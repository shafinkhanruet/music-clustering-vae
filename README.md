#  Unsupervised Music Clustering using Variational Autoencoders

##  Overview

This project presents an **unsupervised framework for music clustering** using **Variational Autoencoders (VAEs)**.  
The objective is to learn **compact and meaningful latent representations** from audio signals and perform clustering in the learned latent space to identify **structural similarities between music tracks**.

The pipeline is **fully reproducible** and designed to be **easily extensible** with additional modalities (such as lyrics or metadata) and alternative clustering algorithms.  
The project is organized into **Easy**, **Medium**, and **Hard** tasks to reflect progressive levels of methodological complexity.

---

## Repository Structure

The repository is organized as follows:

- **Easy_Task/**  
  Audio-only music clustering using a standard Variational Autoencoder (VAE).

- **Medium_Task/**  
  Hybrid clustering using audio features and lyric-based textual features.

- **Hard_Task/**  
  Advanced clustering using a Beta-VAE to encourage disentangled latent
  representations.

Each task directory contains its own implementation scripts and a dedicated
`README.md` with detailed execution instructions.

---
## Methodology Summary

### Audio Feature Extraction
- Audio tracks are processed using Mel-Frequency Cepstral Coefficients (MFCCs).
- Each track is converted into a fixed-length feature vector.

### Representation Learning
- A Variational Autoencoder is trained to learn low-dimensional latent
  representations from audio features.
- A Beta-VAE variant is used in the Hard Task to promote disentangled latent
  factors.

---

###  Clustering

- **K-Means clustering** is applied to the learned latent representations.
- **Number of clusters:** 10
- Clustering quality is evaluated using standard **unsupervised metrics**.

---

## Hybrid Audio–Lyrics Extension (Medium Task)

An extended experiment integrates **textual information** by combining:

- VAE-based **audio latent representations**
- **TF-IDF features** extracted from song lyrics

The resulting **hybrid feature space** is evaluated using multiple clustering algorithms:

- K-Means  
- Agglomerative Clustering  
- DBSCAN  

The hybrid representation **significantly improves clustering quality**, achieving:
- Higher **Silhouette Scores**
- Lower **Davies–Bouldin Index** values  

This demonstrates the benefit of incorporating **semantic information from lyrics** alongside acoustic features.

---

## Advanced Extension: Beta-VAE (Hard Task)

An advanced experiment employs a **Beta-VAE** to encourage **disentangled latent representations** by increasing the weight of the KL-divergence term.

- Clustering performance is **slightly reduced** compared to the standard VAE
- Latent dimensions are **more interpretable and factorized**

These results highlight the trade-off between:
- **Clustering compactness**
- **Latent space interpretability**

This trade-off is a key characteristic of **disentangled representation learning**.

---

## Visualization

A **t-SNE visualization** of the VAE latent space illustrates the separation of
clusters learned by the audio-only model.

![t-SNE Latent Space Visualization](Easy_Task/results/tsne_latent_space.png)


---

##  Dataset Availability

Due to dataset size constraints, **raw audio files are not included** in this repository.

- This project uses the **GTZAN Genre Dataset**, which is publicly available on Kaggle:
[GTZAN Genre Dataset – Kaggle](https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification)
- To reproduce the results, download the dataset separately and place it under the
`data/audio/` directory. The dataset should be organized into genre-specific subfolders (e.g., rock, pop, blues, jazz, classical), where each subfolder contains the corresponding `.wav` audio files for that genre.

Due to size constraints, the raw audio files are not included in this repository.

---

## Results

Quantitative evaluation metrics for all tasks (Easy, Medium, and Hard) are
provided in the consolidated `metrics.txt` file at the repository root.

---

## Reproducibility

All preprocessing, training, and clustering steps are implemented in code and
documented in the task-specific README files. The trained model files (`.pth`)
are included for reference and can also be regenerated by running the provided
scripts.

---

## Conclusion

This project demonstrates the effectiveness of Variational Autoencoders for
unsupervised music clustering. The progression from audio-only clustering to
multimodal learning and disentangled representation learning provides a
comprehensive exploration of modern unsupervised techniques for music analysis.





